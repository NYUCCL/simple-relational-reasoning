{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/guydavidson/.netrc\r\n"
     ]
    }
   ],
   "source": [
    "!wandb login 9676e3cc95066e4865586082971f2653245f09b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import defaultdict, namedtuple\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.special import factorial\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib import path as mpath\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import tabulate\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_TEMPLATE = r'''\\begin{{figure}}[!htb]\n",
    "% \\vspace{{-0.225in}}\n",
    "\\centering\n",
    "\\includegraphics[width=\\linewidth]{{figures/{save_path}}}\n",
    "\\caption{{ {{\\bf FIGURE TITLE.}} FIGURE DESCRIPTION.}}\n",
    "\\label{{fig:{label_name}}}\n",
    "% \\vspace{{-0.2in}}\n",
    "\\end{{figure}}\n",
    "'''\n",
    "WRAPFIGURE_TEMPLATE = r'''\\begin{{wrapfigure}}{{r}}{{0.5\\linewidth}}\n",
    "\\vspace{{-.3in}}\n",
    "\\begin{{spacing}}{{1.0}}\n",
    "\\centering\n",
    "\\includegraphics[width=0.95\\linewidth]{{figures/{save_path}}}\n",
    "\\caption{{ {{\\bf FIGURE TITLE.}} FIGURE DESCRIPTION.}}\n",
    "\\label{{fig:{label_name}}}\n",
    "\\end{{spacing}}\n",
    "% \\vspace{{-.25in}}\n",
    "\\end{{wrapfigure}}'''\n",
    "\n",
    "SAVE_PATH_PREFIX = 'figures'\n",
    "\n",
    "\n",
    "def save_plot(save_path, bbox_inches='tight', should_print=False):\n",
    "    if save_path is not None:\n",
    "        save_path_no_ext = os.path.splitext(save_path)[0]\n",
    "        if should_print:\n",
    "            print('Figure:\\n')\n",
    "            print(FIGURE_TEMPLATE.format(save_path=save_path, label_name=save_path_no_ext.replace('/', '-').replace('_', '-')))\n",
    "            print('\\nWrapfigure:\\n')\n",
    "            print(WRAPFIGURE_TEMPLATE.format(save_path=save_path, label_name=save_path_no_ext.replace('/', '-').replace('_', '-')))\n",
    "            print('')\n",
    "        \n",
    "        if not save_path.startswith(SAVE_PATH_PREFIX):\n",
    "            save_path = os.path.join(SAVE_PATH_PREFIX, save_path)\n",
    "        \n",
    "        folder, filename = os.path.split(save_path)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches=bbox_inches, facecolor=plt.gcf().get_facecolor(), edgecolor='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Condition = namedtuple('Condition', ('paradigm', 'relation', 'model_config', 'object_size', 'neither'))\n",
    "ConditionWithModel = namedtuple('ConditionWithModel', ('model_name', 'paradigm', 'relation', 'model_config', 'object_size', 'neither'))\n",
    "PARADIGM_INDEX = 0\n",
    "RELATION_INDEX = 1\n",
    "MODEL_CONFIG_INDEX = 2\n",
    "OBJECT_SIZE_INDEX = 4\n",
    "NEITHER_INDEX = 7\n",
    "\n",
    "\n",
    "def parse_with_without(raw):\n",
    "    raw = raw.lower().strip()\n",
    "    assert(raw in ('with', 'without'))\n",
    "    return raw == 'with'\n",
    "\n",
    "\n",
    "def name_to_condition(name):\n",
    "    split = name.split('-')\n",
    "    paradigm = split[PARADIGM_INDEX]\n",
    "    relation = split[RELATION_INDEX]\n",
    "    model_config = split[MODEL_CONFIG_INDEX]\n",
    "    object_size = parse_with_without(split[OBJECT_SIZE_INDEX])\n",
    "    neither = parse_with_without(split[NEITHER_INDEX])\n",
    "    return Condition(paradigm=paradigm, relation=relation, model_config=model_config, \n",
    "                     object_size=object_size, neither=neither)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_PATH = './results_cache.pickle'\n",
    "BACKUP_CACHE_PATH = './results_cache_{date}.pickle'\n",
    "\n",
    "\n",
    "def refresh_cache(new_values_dict=None, cache_path=CACHE_PATH):\n",
    "    if new_values_dict is None:\n",
    "        new_values_dict = {}\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as cache_file:\n",
    "            cache = pickle.load(cache_file)\n",
    "    \n",
    "    else:\n",
    "        cache = {}\n",
    "    \n",
    "    cache.update(new_values_dict)\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        os.rename(CACHE_PATH, BACKUP_CACHE_PATH.format(date=datetime.now().strftime('%Y-%m-%d_%H-%M-%S')))\n",
    "\n",
    "    with open(cache_path, 'wb') as cache_file:\n",
    "        pickle.dump(cache, cache_file)\n",
    "\n",
    "    return cache\n",
    " \n",
    "\n",
    "cache = refresh_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['all_results', 'all_results_df', 'all_results_fixed_checkpoints', 'all_results_fixed_checkpoints_df', 'all_results_fixed_cnn', 'all_results_fixed_cnn_df', 'quinn_results', 'quinn_results_df', 'quinn_results_learning_curves', 'quinn_parsed_learning_curve_data'])\n"
     ]
    }
   ],
   "source": [
    "print(cache.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game plan\n",
    "* Iterate through projects \n",
    "* Parse each project conditions from its name\n",
    "* Aggregate each of the following statistics by model:\n",
    "    * Peak accuracy\n",
    "    * Epoch in which peak accuracy takes place\n",
    "    * Epoch above some accuracy threshold\n",
    "    * Did the model overfit (delta between train and val?)\n",
    "* Start examining the effects of different manipulations:\n",
    "    * Choice of relation\n",
    "    * Number of objects\n",
    "    * Size of the training set\n",
    "    * Size of the model (in that one condition I ran larger)\n",
    "* Probably more bar charts tables than anything else, unless we need learning curves\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name_and_seed(run):\n",
    "    name, seed = run.name.rsplit('-', 1)\n",
    "    return name, int(seed)\n",
    "\n",
    "\n",
    "def get_model_name(run):\n",
    "    return get_model_name_and_seed(run)[0]\n",
    "\n",
    "\n",
    "def get_run_seed(run):\n",
    "    return get_model_name_and_seed(run)[1]\n",
    "\n",
    "\n",
    "def fix_project_configs(project):\n",
    "    uri = f'{project.entity}/{project.name}'\n",
    "    print(f'\\tIn project {project.name}:')\n",
    "    runs = api.runs(uri)\n",
    "    configs_by_model = {}\n",
    "    runs_to_fix = defaultdict(list)\n",
    "    \n",
    "#     for run in tqdm(runs, desc='Runs'):\n",
    "    for run in runs:\n",
    "        model_name = get_model_name(run)\n",
    "        if run.config is None or len(run.config) == 0:\n",
    "            runs_to_fix[model_name].append(run)\n",
    "\n",
    "        elif model_name not in configs_by_model:\n",
    "            configs_by_model[model_name] = copy.deepcopy(run.config)\n",
    "    \n",
    "    for model_name in runs_to_fix:\n",
    "        if model_name not in configs_by_model:\n",
    "            raise ValueError(f'In URI {uri}, could not find a config to fix model name {model_name}')\n",
    "            \n",
    "        sample_config = configs_by_model[model_name]\n",
    "        \n",
    "        for run in runs_to_fix[model_name]:\n",
    "            config_copy = copy.deepcopy(sample_config)\n",
    "            config_copy['seed'] = get_run_seed(run)\n",
    "            config_copy['wandb_run_name'] = run.name\n",
    "            run.config = config_copy\n",
    "            run.update()\n",
    "            print(f'\\t\\t{run.name}')\n",
    "            \n",
    "    return runs_to_fix\n",
    "\n",
    "\n",
    "def print_missing_data(project, num_samples=50000):\n",
    "    uri = f'{project.entity}/{project.name}'\n",
    "    print(f'\\tIn project {project.name}:')\n",
    "    runs = api.runs(uri)\n",
    "    configs_by_model = {}\n",
    "    runs_to_fix = defaultdict(list)\n",
    "    \n",
    "#     for run in tqdm(runs, desc='Runs'):\n",
    "    for run in runs:\n",
    "        model_name = get_model_name(run)\n",
    "        history_df = run.history(samples=num_samples)\n",
    "        if 'num_objects_gen_test_acc' not in history_df:\n",
    "            runs_to_fix[model_name].append(run)\n",
    "            print(f'\\t\\t{run.name}')\n",
    "            \n",
    "    return runs_to_fix\n",
    "\n",
    "\n",
    "def print_missing_runs(project, models=('transformer', 'relation-net',\n",
    "                                        'combined-object-mlp', 'simplified-cnn'),\n",
    "                       seed_range=range(100, 110)):\n",
    "    uri = f'{project.entity}/{project.name}'\n",
    "    print(f'\\tIn project {project.name}:')\n",
    "    runs = api.runs(uri)\n",
    "    \n",
    "    missing = {model: list(seed_range) for model in models}\n",
    "    \n",
    "    for run in runs:\n",
    "        model_name, seed = get_model_name_and_seed(run)\n",
    "        if model_name not in missing:\n",
    "            raise ValueError(f'Failed to find {model_name} [came from {url}]')\n",
    "            \n",
    "        if seed in missing[model_name]:\n",
    "            missing[model_name].remove(seed)\n",
    "            if not len(missing[model_name]):\n",
    "                del missing[model_name]\n",
    "                \n",
    "        else:\n",
    "            print(f'\\t\\t Suspected duplicate: {model_name}-{seed}')\n",
    "            \n",
    "    if 'with-object-size' in uri:\n",
    "        del missing['simplified-cnn']\n",
    "            \n",
    "    for model_name, seeds in missing.items():\n",
    "        print(f'\\t\\t{model_name}: {seeds}')\n",
    "        \n",
    "    return missing\n",
    "        \n",
    "\n",
    "def fix_all_projects(entity='simple-relational-reasoning-fixed-checkpoints', \n",
    "                     fix_func=fix_project_configs):\n",
    "    projects = api.projects(entity)\n",
    "    results = {}\n",
    "    \n",
    "    print(projects)\n",
    "    for proj in tqdm([p for p in projects], desc='Projects'):\n",
    "        condition = name_to_condition(proj.name)\n",
    "#         print(proj.name)\n",
    "#         print(condition)\n",
    "        results[condition] = fix_func(proj)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0_projects = set()\n",
    "for proj in api.projects('quinn-relations'):\n",
    "    v0_projects.add(proj.name)\n",
    "    \n",
    "for proj in api.projects('quinn-relations-v1'):\n",
    "    n = proj.name\n",
    "    if n in v0_projects:\n",
    "        v0_projects.remove(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one_or_two_references-above_below-default-models-with-object-size-with-neither',\n",
       " 'one_or_two_references-above_below-default-models-with-object-size-without-neither',\n",
       " 'one_or_two_references-above_below-default-models-without-object-size-with-neither',\n",
       " 'one_or_two_references-above_below-default-models-without-object-size-without-neither',\n",
       " 'one_or_two_references-between-default-models-with-object-size-with-neither',\n",
       " 'one_or_two_references-between-default-models-with-object-size-without-neither',\n",
       " 'one_or_two_references-between-default-models-without-object-size-with-neither',\n",
       " 'one_or_two_references-between-default-models-without-object-size-without-neither'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Projects quinn-relations-v1>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6663d630b4bc4c1297534c0a113ae654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Projects'), FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIn project inductive_bias-between-default-models-with-object-size-with-neither:\n",
      "\tIn project inductive_bias-between-default-models-with-object-size-without-neither:\n",
      "\tIn project inductive_bias-between-default-models-without-object-size-with-neither:\n",
      "\tIn project inductive_bias-between-default-models-without-object-size-without-neither:\n",
      "\tIn project inductive_bias-above_below-default-models-with-object-size-with-neither:\n",
      "\tIn project inductive_bias-above_below-default-models-with-object-size-without-neither:\n",
      "\tIn project inductive_bias-above_below-default-models-without-object-size-with-neither:\n",
      "\tIn project inductive_bias-above_below-default-models-without-object-size-without-neither:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{Condition(paradigm='inductive_bias', relation='between', model_config='default', object_size=True, neither=True): {},\n",
       " Condition(paradigm='inductive_bias', relation='between', model_config='default', object_size=True, neither=False): {},\n",
       " Condition(paradigm='inductive_bias', relation='between', model_config='default', object_size=False, neither=True): {},\n",
       " Condition(paradigm='inductive_bias', relation='between', model_config='default', object_size=False, neither=False): {},\n",
       " Condition(paradigm='inductive_bias', relation='above_below', model_config='default', object_size=True, neither=True): {},\n",
       " Condition(paradigm='inductive_bias', relation='above_below', model_config='default', object_size=True, neither=False): {},\n",
       " Condition(paradigm='inductive_bias', relation='above_below', model_config='default', object_size=False, neither=True): {},\n",
       " Condition(paradigm='inductive_bias', relation='above_below', model_config='default', object_size=False, neither=False): {}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_all_projects('quinn-relations-v1', print_missing_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix_results = fix_all_projects(fix_func=print_missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix_results = fix_all_projects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def parse_single_key(history_df, run_results, key, baseline_key='train_acc', accuracy_threshold=0.95):\n",
    "    max_index = history_df[key].idxmax()\n",
    "    run_results[key] = history_df[key][max_index]\n",
    "    run_results[f'{key}_step'] = history_df.global_step[max_index]\n",
    "    \n",
    "    accuracy_above_treshold_index = (history_df[key] >= accuracy_threshold).idxmax()\n",
    "    threshold_step = None\n",
    "    if accuracy_above_treshold_index > 0:  # 0 means never went above treshold\n",
    "        threshold_step = history_df.global_step[accuracy_above_treshold_index]\n",
    "    run_results[f'{key}_threshold_step'] = threshold_step\n",
    "    \n",
    "    if key != baseline_key:\n",
    "        run_results[f'{key}_diff'] = history_df[baseline_key].max() - run_results[key]\n",
    "        \n",
    "        \n",
    "def parse_single_key_learning_curves(history_df, run_results, key, baseline_key='train_acc', accuracy_threshold=0.95):\n",
    "    series = history_df[key]\n",
    "    run_results[key] = np.array(series[series.notnull()])\n",
    "\n",
    "\n",
    "def parse_run(run, num_samples=50000, baseline_key='train_acc', accuracy_threshold=0.95,\n",
    "              parse_key_func=parse_single_key):\n",
    "    history_df = run.history(samples=num_samples)\n",
    "    run_results = {}\n",
    "    \n",
    "    accuracy_keys = [key for key in history_df.keys() if 'acc' in key]\n",
    "    for key in accuracy_keys:\n",
    "        parse_key_func(history_df, run_results, key, \n",
    "                       baseline_key=baseline_key, accuracy_threshold=accuracy_threshold)\n",
    "    \n",
    "    return run_results\n",
    "    \n",
    "    \n",
    "def parse_project(project, parse_key_func=parse_single_key):\n",
    "    runs = api.runs(f'{project.entity}/{project.name}')\n",
    "    results_by_model = defaultdict(list)\n",
    "    \n",
    "    for run in runs:  # tqdm(runs, desc='Runs'):\n",
    "        try:\n",
    "            seed_split_index = run.name.rindex('-')\n",
    "            model_name = run.name[:seed_split_index]\n",
    "            seed = int(run.name[seed_split_index + 1:])\n",
    "\n",
    "            run_results = parse_run(run, parse_key_func=parse_key_func)\n",
    "            run_results['seed'] = seed\n",
    "            run_results['total_params'] = run.config['total_params']\n",
    "            results_by_model[model_name].append(run_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(run)\n",
    "            print(run.name)\n",
    "            print(run.config)\n",
    "            raise(e)\n",
    "        \n",
    "    return results_by_model\n",
    "        \n",
    "        \n",
    "def parse_all_projects(entity='quinn-relations', parse_key_func=parse_single_key):\n",
    "    projects = api.projects(entity)\n",
    "    results = {}\n",
    "    \n",
    "    for proj in tqdm([p for p in projects], desc='Projects'):\n",
    "        condition = name_to_condition(proj.name)\n",
    "#         print(proj.name)\n",
    "#         print(condition)\n",
    "        results[condition] = parse_project(proj, parse_key_func=parse_key_func)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "if 'quinn_results' not in cache:\n",
    "    cache['quinn_results'] = parse_all_projects()\n",
    "    cache = refresh_cache(cache)\n",
    "    \n",
    "quinn_results = cache['quinn_results']\n",
    "\n",
    "if 'quinn_results_learning_curves' not in cache:\n",
    "    cache['quinn_results_learning_curves'] = parse_all_projects(parse_key_func=parse_single_key_learning_curves)\n",
    "    cache = refresh_cache(cache)\n",
    "    \n",
    "quinn_results_learning_curves = cache['quinn_results_learning_curves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da2d017d9f34467af19504ffb534ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Projects'), FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce0e0b808ad42baac72a1faf7d02dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Projects'), FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if 'quinn_results_v1' not in cache:\n",
    "    cache['quinn_results_v1'] = parse_all_projects(entity='quinn-relations-v1')\n",
    "    cache = refresh_cache(cache)\n",
    "    \n",
    "quinn_results_v1 = cache['quinn_results_v1']\n",
    "\n",
    "if 'quinn_results_learning_curves_v1' not in cache:\n",
    "    cache['quinn_results_learning_curves_v1'] = parse_all_projects(entity='quinn-relations-v1', parse_key_func=parse_single_key_learning_curves)\n",
    "    cache = refresh_cache(cache)\n",
    "    \n",
    "quinn_results_learning_curves_v1 = cache['quinn_results_learning_curves_v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40058c4a0a49487d81c4156d94616d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Condition'), FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CONDITION_FIELDS = list(Condition._fields)\n",
    "COLUMNS = CONDITION_FIELDS + ['model_name'] \n",
    "\n",
    "\n",
    "def get_all_columns(results_dict):\n",
    "    columns = set()\n",
    "    for condition_to_model_dict in results_dict.values():\n",
    "        result_list = next(iter(condition_to_model_dict.values()))\n",
    "        columns.update(result_list[0].keys())\n",
    "    return columns\n",
    "\n",
    "\n",
    "def results_dict_to_df(results_dict):\n",
    "    rows = []\n",
    "    all_columns = list(get_all_columns(results_dict))\n",
    "    header_columns = COLUMNS + all_columns\n",
    "    for condition, results_by_model in tqdm(results_dict.items(), desc='Condition'):\n",
    "        condition_row_prefix = list(condition)\n",
    "#         print(condition_row_prefix)\n",
    "        for model_name, results_by_seed in results_by_model.items():  # tqdm(results_by_model.items(), desc='Result'):\n",
    "            for run_results in results_by_seed:\n",
    "                run_result_list = [run_results[field] if field in run_results else None for field in all_columns]\n",
    "                rows.append(condition_row_prefix + [model_name] + run_result_list)\n",
    "                \n",
    "    return pd.DataFrame(rows, columns=header_columns)\n",
    "\n",
    "\n",
    "if 'quinn_results_df' not in cache:\n",
    "    cache['quinn_results_df'] = results_dict_to_df(quinn_results)\n",
    "    cache = refresh_cache(cache)\n",
    "    \n",
    "quinn_results_df = cache['quinn_results_df']\n",
    "\n",
    "if 'quinn_results_v1_df' not in cache:\n",
    "    cache['quinn_results_v1_df'] = results_dict_to_df(quinn_results_v1)\n",
    "    cache = refresh_cache(cache)\n",
    "    \n",
    "quinn_results_v1_df = cache['quinn_results_v1_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47dfa85e4f804eaaaf8796fe35162e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Condition'), FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_learning_curve_results(results_dict):\n",
    "    full_results = {}\n",
    "    for condition, condition_results in tqdm(results_dict.items(), desc='Condition'):\n",
    "        condition_results = results_dict[condition]\n",
    "        \n",
    "        for model_type in condition_results:\n",
    "            model_type_results = condition_results[model_type]\n",
    "            condition_and_model_results = defaultdict(list)\n",
    "            condition_and_model_arrays = {}\n",
    "            \n",
    "            for seed_results in model_type_results:\n",
    "                for result_key, result_value in seed_results.items():\n",
    "                    if isinstance(result_value, np.ndarray):\n",
    "                        condition_and_model_results[result_key].append(result_value)\n",
    "                    \n",
    "            # TODO: combine to a 2d array\n",
    "            for result_key, result_values in condition_and_model_results.items():\n",
    "                max_len = max([len(values) for values in result_values])\n",
    "                padded_values = [np.pad(values, (0, max_len - len(values)), constant_values=np.nan)\n",
    "                                 for values in result_values]\n",
    "                condition_and_model_arrays[result_key] = np.stack(padded_values)\n",
    "            \n",
    "            # TODO: create a key from condition and model type\n",
    "            condition_with_model = ConditionWithModel(model_type, *condition)\n",
    "            full_results[condition_with_model] = condition_and_model_arrays\n",
    "            \n",
    "    return full_results\n",
    "            \n",
    "if 'quinn_parsed_learning_curve_data' not in cache:\n",
    "    cache['quinn_parsed_learning_curve_data'] = parse_learning_curve_results(quinn_results_learning_curves)\n",
    "    cache = refresh_cache(cache)\n",
    "    \n",
    "quinn_parsed_learning_curve_data = cache['quinn_parsed_learning_curve_data']\n",
    "\n",
    "if 'quinn_parsed_learning_curve_data_v1' not in cache:\n",
    "    cache['quinn_parsed_learning_curve_data_v1'] = parse_learning_curve_results(quinn_results_learning_curves_v1)\n",
    "    cache = refresh_cache(cache)\n",
    "    \n",
    "quinn_parsed_learning_curve_data = cache['quinn_parsed_learning_curve_data_v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quinn_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quinn_results_df.model_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quinn_results_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quinn_results_df[[key for key in quinn_results_df.keys() if ('acc' in key or key == 'paradigm') and ('step' not in key)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quinn_results_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting thoughts\n",
    "* I want a single plotting function that will generate a group of bar charts\n",
    "* Steps:\n",
    "    1. Filter out particular parts of the dataframe.\n",
    "    2. Group by one or more fields.\n",
    "    3. Aggregate (mean, sd) the metric of choice.\n",
    "    4. Group the bars by one of the group-by fields. \n",
    "* I probably want some way to specify formatting by the individual groups left (for example, a color for each model, a striping for each relation, ...)\n",
    "\n",
    "**TODO:** \n",
    "* Pretitfy names\n",
    "* Model (or other field orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ORDERS = dict(\n",
    "    model_name=['combined-object-mlp', 'simplified-cnn', 'relation-net', 'transformer'],\n",
    "    relation=['above_below', 'between'],\n",
    "    one_or_two_references=['train_acc', 'train_reference_test_target_acc', \n",
    "                           'test_reference_train_target_acc', 'test_reference_test_target_acc'],\n",
    "    inductive_bias=['train_acc', 'train_reference_test_target_acc', 'test_reference_train_target_acc', \n",
    "                    'test_reference_test_target_acc',  'train_reference_middle_target_acc', \n",
    "                    'test_reference_middle_target_acc']\n",
    ")\n",
    "DEFAULT_COLORMAP = plt.get_cmap('Dark2')\n",
    "DEFAULT_BAR_KWARGS_BY_FIELD = defaultdict(lambda: defaultdict(dict))\n",
    "DEFAULT_BAR_KWARGS_BY_FIELD['model_name'] = {name: dict(facecolor=DEFAULT_COLORMAP(i))\n",
    "                                             for i, name in enumerate(DEFAULT_ORDERS['model_name'])}\n",
    "DEFAULT_BAR_KWARGS_BY_FIELD['num_objects'] = {5: {'hatch': ''}, 10: {'hatch': '/'}}\n",
    "\n",
    "DEFAULT_BAR_KWARGS = dict(edgecolor='black')\n",
    "\n",
    "SHADE_BAR_KWARGS = dict(alpha=0.25)\n",
    "\n",
    "DEFAULT_TEXT_KWARGS = dict(fontsize=16)\n",
    "\n",
    "DEFAULT_YLIM = (0, 1.05)\n",
    "\n",
    "PRETTY_NAMES = {\n",
    "    'cnn': 'CNN',\n",
    "    'simplified-cnn': 'SimplifiedCNN',\n",
    "    'combined-object-mlp': 'MLP',\n",
    "    'relation-net': 'RelationNet',\n",
    "    'transformer': 'Transformer',\n",
    "    'train_acc': 'Train Accuracy',\n",
    "    'train_reference_test_target_acc': 'Train Ref\\nTest Target',\n",
    "    'test_reference_train_target_acc': 'Test Ref\\nTrain Target',\n",
    "    'test_reference_test_target_acc': 'Test Ref\\nTest Target',\n",
    "    'train_reference_middle_target_acc': 'Train Ref\\nMid Target',\n",
    "    'test_reference_middle_target_acc': 'Test Ref\\nMid Target',\n",
    "    'model_config': 'Model Configuration Set',\n",
    "    'num_objects_gen_test_acc': 'Generalization Test Accuracy',\n",
    "    '5': '5 Objects',\n",
    "    '10': '10 Objects',\n",
    "}\n",
    "\n",
    "\n",
    "def prettify(text):\n",
    "    if isinstance(text, int) and text > 1000:\n",
    "        return f'{text // 1000}k ($2^{{ {int(np.log2(text))} }}$)'\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    if text in PRETTY_NAMES:\n",
    "        return PRETTY_NAMES[text]\n",
    "    \n",
    "    for key in PRETTY_NAMES:\n",
    "        if key in text:\n",
    "            return PRETTY_NAMES[key]\n",
    "\n",
    "    return text.lower().replace('_', ' ').title()\n",
    "\n",
    "\n",
    "def filter_and_group(df, filter_dict, group_by_fields, \n",
    "                     orders=DEFAULT_ORDERS):\n",
    "    filtered_df = df.copy(deep=True)\n",
    "    group_by_fields = group_by_fields[:]\n",
    "    if 'metric' in group_by_fields:\n",
    "        group_by_fields.remove('metric')\n",
    "    \n",
    "    if filter_dict is not None:\n",
    "        for filter_name, filter_value in filter_dict.items():\n",
    "            if isinstance(filter_value, (list, tuple)):\n",
    "                filtered_df = filtered_df[filtered_df[filter_name].isin(filter_value)]\n",
    "                if filter_name in orders:\n",
    "                    orders[filter_name] = list(filter(lambda v: v in filter_value, orders[filter_name]))\n",
    "            else:\n",
    "                filtered_df = filtered_df[filtered_df[filter_name].eq(filter_value)]\n",
    "            \n",
    "    return filtered_df.groupby(group_by_fields)\n",
    "\n",
    "\n",
    "def create_mean_and_std(df, filter_dict, group_by_fields, metric,\n",
    "                     orders=DEFAULT_ORDERS, sem=False, metric_suffix=None,\n",
    "                     mean_func=None):\n",
    "    orders = copy.deepcopy(orders)\n",
    "    if metric is None:\n",
    "        if 'paradigm' not in filter_dict:\n",
    "            raise ValueError('metric=None requires specifying a paradigm in filter_dict')\n",
    "        \n",
    "        metric = orders[filter_dict['paradigm']]\n",
    "        if metric_suffix is not None:\n",
    "            metric = [m + metric_suffix for m in metric]\n",
    "            \n",
    "        orders['metric'] = metric\n",
    "    \n",
    "    grouped_df = filter_and_group(df, filter_dict, group_by_fields, orders)\n",
    "    if mean_func is None:\n",
    "        mean = grouped_df[metric].mean()\n",
    "    else:\n",
    "        mean = mean_func(grouped_df[metric])\n",
    "        \n",
    "    std = grouped_df[metric].std()    \n",
    "    if sem:\n",
    "        std /= np.sqrt(grouped_df[metric].count())\n",
    "        \n",
    "    return mean, std, grouped_df, orders\n",
    "\n",
    "\n",
    "\n",
    "def create_bar_chart(df, filter_dict, group_by_fields, metric,\n",
    "                     orders=DEFAULT_ORDERS, sem=False, metric_suffix=None,\n",
    "                     mean_func=None ,title=None,\n",
    "                     bar_kwargs_by_field=DEFAULT_BAR_KWARGS_BY_FIELD,\n",
    "                     bar_width=0.2, bar_spacing=0.5, default_bar_kwargs=DEFAULT_BAR_KWARGS,\n",
    "                     text_kwargs=DEFAULT_TEXT_KWARGS, shade_metric=None, \n",
    "                     shade_bar_kwargs=SHADE_BAR_KWARGS, plot_shade_std=False,\n",
    "                     add_chance_hline=True, plot_std=True, ylim=DEFAULT_YLIM, ylabel='Accuracy', \n",
    "                     legend_loc='best', save_path=None, save_should_print=False, \n",
    "                     ax=None, legend=True, should_show=False):\n",
    "    \n",
    "    group_by_fields = list(group_by_fields)\n",
    "    \n",
    "    mean, std, grouped_df, orders = create_mean_and_std(\n",
    "        df, filter_dict, group_by_fields, metric,\n",
    "        orders, sem, metric_suffix, mean_func)\n",
    "    \n",
    "    major_group_by = group_by_fields[0]\n",
    "    minor_group_by = group_by_fields[1:]\n",
    "    \n",
    "    if default_bar_kwargs is None:\n",
    "        default_bar_kwargs = dict()\n",
    "    \n",
    "    if shade_bar_kwargs is None:\n",
    "        shade_bar_kwargs = dict()\n",
    "    \n",
    "    shade_bar_kwargs_temp = copy.deepcopy(default_bar_kwargs)\n",
    "    shade_bar_kwargs_temp.update(shade_bar_kwargs)\n",
    "    shade_bar_kwargs = shade_bar_kwargs_temp\n",
    "    \n",
    "    if shade_metric is not None:\n",
    "        shade_mean = grouped_df[shade_metric].mean()\n",
    "        shade_std = grouped_df[shade_metric].std()    \n",
    "        if sem:\n",
    "            shade_std /= np.sqrt(grouped_df[shade_metric].count())\n",
    "\n",
    "    if major_group_by in orders:\n",
    "        major_group_values = orders[major_group_by]\n",
    "    else:\n",
    "        major_group_values = mean.index.unique(level=major_group_by)\n",
    "        \n",
    "    minor_group_values_list = []\n",
    "    \n",
    "    for minor_field_name in minor_group_by:\n",
    "        if minor_field_name in orders:\n",
    "            minor_group_values_list.append(orders[minor_field_name])\n",
    "        else:\n",
    "            minor_group_values_list.append(mean.index.unique(level=minor_field_name))\n",
    "    \n",
    "    major_kwargs = bar_kwargs_by_field[major_group_by]\n",
    "    minor_kwargs_list = [bar_kwargs_by_field[minor_field_name] for minor_field_name in minor_group_by]\n",
    "    \n",
    "    if ax is None:\n",
    "        figure = plt.figure(figsize=(8, 6))\n",
    "        ax = plt.gca()\n",
    "        should_show = True\n",
    "        \n",
    "    x = 0\n",
    "    \n",
    "    for major_level_value in major_group_values:\n",
    "        major_level_kwargs = major_kwargs[major_level_value]\n",
    "        \n",
    "        for minor_level_value_combination in itertools.product(*minor_group_values_list):\n",
    "            combined_minor_kwargs = {}\n",
    "            for i, value in enumerate(minor_level_value_combination):\\\n",
    "                combined_minor_kwargs.update(minor_kwargs_list[i][value])\n",
    "                \n",
    "            major_and_minor_key = (major_level_value, *minor_level_value_combination)\n",
    "            \n",
    "            if shade_metric:  # intentionally shade before the real bars\n",
    "                shade_m = shade_mean.loc[major_and_minor_key]\n",
    "                if plot_shade_std:\n",
    "                    shade_s = shade_std.loc[major_and_minor_key]\n",
    "                else:\n",
    "                    shade_s = 0\n",
    "                plt.bar(x, shade_m, yerr=shade_s, width=bar_width, **major_level_kwargs, \n",
    "                        **combined_minor_kwargs, **shade_bar_kwargs)\n",
    "            \n",
    "            if major_group_by == 'metric':\n",
    "                major_and_minor_key = major_and_minor_key[::-1]\n",
    "                \n",
    "            m = mean.loc[major_and_minor_key]\n",
    "            if plot_std:\n",
    "                s = std.loc[major_and_minor_key]\n",
    "            else:\n",
    "                s = None\n",
    "            \n",
    "            ax.bar(x, m, yerr=s, width=bar_width, **major_level_kwargs, \n",
    "                    **combined_minor_kwargs, **default_bar_kwargs)\n",
    "\n",
    "            x += bar_width\n",
    "        \n",
    "        x += bar_spacing\n",
    "        \n",
    "    minor_group_length = np.product([len(values) for values in minor_group_values_list])\n",
    "    x_tick_locations = np.arange(len(major_group_values)) * (bar_spacing + bar_width * minor_group_length) +\\\n",
    "                        bar_width * (minor_group_length / 2 - 0.5)\n",
    "    xtick_text_kwargs = text_kwargs.copy()\n",
    "    if len(major_group_values) > 4:\n",
    "        xtick_text_kwargs['fontsize'] -= 4\n",
    "    ax.set_xticks(x_tick_locations)\n",
    "    ax.set_xticklabels([prettify(val) for val in major_group_values], fontdict=xtick_text_kwargs)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    if add_chance_hline:\n",
    "        xlim = plt.xlim()\n",
    "        ax.hlines(0.5, *xlim, linestyle='--', alpha=0.5)\n",
    "        ax.set_xlim(*xlim)\n",
    "        \n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim)\n",
    "\n",
    "    ax.set_xlabel(prettify(major_group_by), **text_kwargs)\n",
    "    if major_group_by == 'metric':\n",
    "        ax.set_ylabel(ylabel, **text_kwargs)\n",
    "    else:\n",
    "        ax.set_ylabel(prettify(metric), **text_kwargs)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    \n",
    "    patches = []\n",
    "    ncol = 0\n",
    "    for kwarg_set, field_name in zip([major_kwargs] + minor_kwargs_list, group_by_fields):\n",
    "        if any([len(val) > 0 for val in kwarg_set.values()]):\n",
    "            ncol += 1\n",
    "            for field_value in kwarg_set:\n",
    "                if field_name in filter_dict and field_value not in filter_dict[field_name]:\n",
    "                    continue\n",
    "                \n",
    "                patch_kwargs = dict(facecolor='none', edgecolor='black')\n",
    "                patch_kwargs.update(kwarg_set[field_value])\n",
    "                patches.append(matplotlib.patches.Patch(**patch_kwargs, label=prettify(field_value)))\n",
    "    \n",
    "    if len(patches) > 0 and legend: \n",
    "        ax.legend(handles=patches, loc=legend_loc, ncol=ncol, fontsize=12)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        save_plot(save_path, should_print=save_should_print)\n",
    "    \n",
    "    if should_show:\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "DEFUALT_OUTCOME_FIELDS = ('max_val_acc', 'num_objects_gen_test_acc') \n",
    "\n",
    "\n",
    "def generalization_test_table(df, filter_dict, group_by_fields, \n",
    "                              outcome_fields=DEFUALT_OUTCOME_FIELDS,\n",
    "                              orders=DEFAULT_ORDERS, baseline=False):\n",
    "    if 'num_objects' not in group_by_fields:\n",
    "        group_by_fields = (*group_by_fields, 'num_objects')\n",
    "        \n",
    "    grouped = filter_and_group(df, filter_dict, group_by_fields, orders=orders)\n",
    "    means = grouped[outcome_fields].mean()\n",
    "    variances = grouped[outcome_fields].var()\n",
    "    counts = grouped[outcome_fields].count()\n",
    "    \n",
    "    results_dict = defaultdict(dict)\n",
    "    \n",
    "    for key in means.index:\n",
    "        key_without_num_objects = key[:-1] \n",
    "        num_objects = key[-1]\n",
    "        \n",
    "        if baseline:\n",
    "            baseline_col_name = f'Train \\\\& Test {num_objects}'\n",
    "            results_dict[key_without_num_objects][baseline_col_name] = f'${means.loc[key][outcome_fields[0]]:.3f}$'\n",
    "            \n",
    "        \n",
    "        mean_diff = means.loc[key][outcome_fields[0]] - means.loc[key][outcome_fields[1]] \n",
    "        diff_sd = np.sqrt((variances.loc[key][outcome_fields[0]] / counts.loc[key][outcome_fields[0]]) +\n",
    "                          (variances.loc[key][outcome_fields[1]] / counts.loc[key][outcome_fields[1]]))\n",
    "        \n",
    "        col_name = f'Train {num_objects} $\\\\rightarrow$ Test {15 - num_objects}'\n",
    "        results_dict[key_without_num_objects][col_name] = f'${mean_diff:.3f} \\\\pm {diff_sd:.3f}$'\n",
    "        \n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(results_dict, orient='index')  \n",
    "    results_df = results_df.rename(index=PRETTY_NAMES)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DEFAULT_KWARGS = dict(linewidth=3)\n",
    "DEFAULT_KWARG_NAME_MAPPING = dict(facecolor='color')\n",
    "\n",
    "def filter_and_group_learning_curve_data(learning_curve_data, filter_dict,\n",
    "                                         group_by_fields, orders=DEFAULT_ORDERS):\n",
    "    grouped_and_filtered_result = defaultdict(dict)\n",
    "    \n",
    "    filtered_keys = list(quinn_parsed_learning_curve_data.keys())\n",
    "    \n",
    "    group_by_fields = list(group_by_fields)[:]\n",
    "    if 'metric' in group_by_fields:\n",
    "        group_by_fields.remove('metric')\n",
    "    \n",
    "    if filter_dict is not None:\n",
    "        for filter_name, filter_value in filter_dict.items():\n",
    "            if isinstance(filter_value, (list, tuple)):\n",
    "                filtered_keys = [key for key in filtered_keys if getattr(key, filter_name) in filter_value]\n",
    "                if filter_name in orders:\n",
    "                    orders[filter_name] = list(filter(lambda v: v in filter_value, orders[filter_name]))\n",
    "            else:\n",
    "                filtered_keys = [key for key in filtered_keys if getattr(key, filter_name) == filter_value]\n",
    "    \n",
    "    group_by_field_values = defaultdict(set)\n",
    "    for group_by_field in group_by_fields:\n",
    "        for key in filtered_keys:\n",
    "            group_by_field_values[group_by_field].add(getattr(key, group_by_field))\n",
    "            \n",
    "    group_by_field_values = {key: list(value) for key, value in group_by_field_values.items()}\n",
    "    \n",
    "    for group_by_field_value_set in itertools.product(*group_by_field_values.values()):\n",
    "        relevant_keys = [key for key in filtered_keys if \n",
    "                         all([getattr(key, prop) == value for (prop, value) in zip(group_by_fields, group_by_field_value_set)])]\n",
    "        \n",
    "        for metric in learning_curve_data[relevant_keys[0]].keys():\n",
    "            metric_values = [learning_curve_data[key][metric] for key in relevant_keys]\n",
    "            max_len = max([values.shape[1] for values in metric_values])\n",
    "            padded_metric_values = [np.pad(values, ((0, 0), (0, max_len - values.shape[1])), constant_values=np.nan)\n",
    "                                   for values in metric_values]\n",
    "            \n",
    "            metric_matrix = np.concatenate(padded_metric_values, axis=0)                             \n",
    "            grouped_and_filtered_result[group_by_field_value_set][metric] = metric_matrix\n",
    "        \n",
    "    return grouped_and_filtered_result\n",
    "\n",
    "\n",
    "def create_learning_curves(data, filter_dict, group_by_fields, metrics,\n",
    "                           orders=DEFAULT_ORDERS, sem=False, last_value_pad=False,\n",
    "                           title=None, kwargs_by_field=DEFAULT_BAR_KWARGS_BY_FIELD,\n",
    "                           default_kwargs=DEFAULT_KWARGS, text_kwargs=DEFAULT_TEXT_KWARGS,\n",
    "                           kwarg_name_mapping=DEFAULT_KWARG_NAME_MAPPING,\n",
    "                           add_chance_hline=True, plot_std=True, ylim=DEFAULT_YLIM, \n",
    "                           ylabel='Accuracy', xlabel='Step',\n",
    "                           legend_loc='best', save_path=None, save_should_print=False, \n",
    "                           ax=None, legend=True, should_show=False):\n",
    "    \n",
    "    group_by_fields = list(group_by_fields)[:]\n",
    "    if 'metric' in group_by_fields:\n",
    "        group_by_fields.remove('metric')\n",
    "    \n",
    "    filtered_and_grouped_data = filter_and_group_learning_curve_data(data, filter_dict, group_by_fields, orders)\n",
    "    \n",
    "    if default_kwargs is None:\n",
    "        default_kwargs = dict()\n",
    "        \n",
    "    if ax is None:\n",
    "        figure = plt.figure(figsize=(8, 6))\n",
    "        ax = plt.gca()\n",
    "        should_show = True\n",
    "    \n",
    "    keys_to_kwargs = {}\n",
    "    \n",
    "    for final_key in filtered_and_grouped_data:\n",
    "        for metric in metrics:\n",
    "            kwargs = default_kwargs.copy()\n",
    "            for field_name, value in zip(group_by_fields, final_key):\n",
    "                if field_name in kwargs_by_field:\n",
    "                    kwargs.update(kwargs_by_field[field_name][value])\n",
    "                    \n",
    "            for kwarg_key in kwarg_name_mapping:\n",
    "                if kwarg_key in kwargs:\n",
    "                    kwargs[kwarg_name_mapping[kwarg_key]] = kwargs.pop(kwarg_key)\n",
    "                    \n",
    "            keys_to_kwargs[final_key] = kwargs\n",
    "            \n",
    "            metric_values = filtered_and_grouped_data[final_key][metric]\n",
    "            \n",
    "            if last_value_pad:\n",
    "                metric_values = np.copy(metric_values)\n",
    "                max_non_nan = metric_values.shape[1] - (~np.isnan(metric_values))[:, ::-1].argmax(1) - 1\n",
    "                print(max_non_nan.mean(), max_non_nan.std())\n",
    "                for r, c in zip(np.arange(max_non_nan.shape[0]), max_non_nan):\n",
    "                    metric_values[r, c:] = metric_values[r, c]\n",
    "            \n",
    "            m = np.nanmean(metric_values, 0)\n",
    "            x = np.arange(len(m))\n",
    "            ax.plot(x, m, **kwargs)\n",
    "            \n",
    "            if plot_std:\n",
    "                s = np.nanstd(metric_values, 0)\n",
    "                if sem:\n",
    "                    s /= np.sqrt(metric_values.shape[0])\n",
    "                    \n",
    "                ax.fill_between(x, m - s, m + s, alpha=0.5, **kwargs)\n",
    "            \n",
    "    if add_chance_hline:\n",
    "        xlim = plt.xlim()\n",
    "        ax.hlines(0.5, *xlim, linestyle='--', alpha=0.5)\n",
    "        ax.set_xlim(*xlim)\n",
    "        \n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim)\n",
    "\n",
    "    ax.set_xlabel(xlabel, **text_kwargs)\n",
    "    ax.set_ylabel(ylabel, **text_kwargs)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    \n",
    "    patches = []\n",
    "    ncol = 1\n",
    "    for (key, kwargs) in  keys_to_kwargs.items():\n",
    "        patch_kwargs = dict(facecolor='none', edgecolor='black')\n",
    "        patch_kwargs.update(kwargs)\n",
    "        patches.append(matplotlib.patches.Patch(**patch_kwargs, label='-'.join([prettify(k) for k in key])))\n",
    "    \n",
    "    if len(patches) > 0 and legend: \n",
    "        ax.legend(handles=patches, loc=legend_loc, ncol=ncol, fontsize=12)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        save_plot(save_path, should_print=save_should_print)\n",
    "    \n",
    "    if should_show:\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 6))\n",
    "\n",
    "above_below_ax = plt.subplot(1, 2, 1)\n",
    "create_learning_curves(quinn_parsed_learning_curve_data, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'), plot_std=False, # last_value_pad=True,\n",
    "                 metrics=('train_acc',), legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two References paradigm, Above vs. Below', ax=above_below_ax\n",
    "                )\n",
    "\n",
    "between_ax = plt.subplot(1, 2, 2)\n",
    "create_learning_curves(quinn_parsed_learning_curve_data, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'), plot_std=False,\n",
    "                 metrics=('train_acc',), legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two References paradigm, Between', ax=between_ax\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_and_grouped_data = filter_and_group_learning_curve_data(quinn_parsed_learning_curve_data, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std, grouped_df, orders = create_mean_and_std(\n",
    "        quinn_results_v1_df,  filter_dict=dict(paradigm='one_or_two_references', model_config='default', relation='above_below'), \n",
    "                 group_by_fields=['metric', 'model_name'],\n",
    "                 metric=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['inductive_bias'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quinn_results_v1_df.paradigm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'combined-object-mlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'combined-object-mlp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ad38bceb5935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mabove_below_ax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m create_bar_chart(quinn_results_v1_df, \n\u001b[0m\u001b[1;32m      5\u001b[0m                  \u001b[0mfilter_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparadigm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'one_or_two_references'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'above_below'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                  \u001b[0mgroup_by_fields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metric'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-0b4f87fb750c>\u001b[0m in \u001b[0;36mcreate_bar_chart\u001b[0;34m(df, filter_dict, group_by_fields, metric, orders, sem, metric_suffix, mean_func, title, bar_kwargs_by_field, bar_width, bar_spacing, default_bar_kwargs, text_kwargs, shade_metric, shade_bar_kwargs, plot_shade_std, add_chance_hline, plot_std, ylim, ylabel, legend_loc, save_path, save_should_print, ax, legend, should_show)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mmajor_and_minor_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmajor_and_minor_key\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmajor_and_minor_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mplot_std\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmajor_and_minor_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    871\u001b[0m                     \u001b[0;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    784\u001b[0m                 \u001b[0;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0;31m# We should never have a scalar section here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3489\u001b[0m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3491\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3493\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'combined-object-mlp'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(17, 6))\n",
    "\n",
    "above_below_ax = plt.subplot(1, 2, 1)\n",
    "create_bar_chart(quinn_results_v1_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two References paradigm, Above vs. Below', ax=above_below_ax\n",
    "                )\n",
    "\n",
    "between_ax = plt.subplot(1, 2, 2)\n",
    "create_bar_chart(quinn_results_v1_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two References paradigm, Between', ax=between_ax\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 16))\n",
    "\n",
    "above_below_prop_ax = plt.subplot(2, 2, 1)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, plot_std=False, metric_suffix='_threshold_step', \n",
    "                 legend_loc='lower right', ylim=None, add_chance_hline=False,\n",
    "                 mean_func=lambda x: x.count() / 40, ylabel='Proportion of runs reaching 95% accuracy',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two | Above vs. Below | % to 95% acc ', ax=above_below_prop_ax\n",
    "                )\n",
    "\n",
    "above_below_step_ax = plt.subplot(2, 2, 2)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, plot_std=False, metric_suffix='_threshold_step', \n",
    "                 legend_loc='upper right', ylim=None, add_chance_hline=False,\n",
    "#                  mean_func=lambda x: x.count() / 40, \n",
    "                 ylabel='Mean global step to 95% accuracy',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two | Above vs. Below | Step to 95% acc ', ax=above_below_step_ax\n",
    "                )\n",
    "\n",
    "between_prop_ax = plt.subplot(2, 2, 3)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, plot_std=False, metric_suffix='_threshold_step', \n",
    "                 legend_loc='lower right', ylim=None, add_chance_hline=False,\n",
    "                 mean_func=lambda x: x.count() / 40, ylabel='Proportion of runs reaching 95% accuracy',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two | Between | % to 95% acc', ax=between_prop_ax\n",
    "                )\n",
    "\n",
    "between_step_ax = plt.subplot(2, 2, 4)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, plot_std=False, metric_suffix='_threshold_step', \n",
    "                 legend_loc='upper right', ylim=None, add_chance_hline=False,\n",
    "#                  mean_func=lambda x: x.count() / 40, \n",
    "                 ylabel='Mean global step to 95% accuracy',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two | Between | Step to 95% acc ', ax=between_step_ax\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we add neither?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 16))\n",
    "\n",
    "above_below_no_neither_ax = plt.subplot(2, 2, 1)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', \n",
    "                                  neither=False, relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two | Above vs. Below | No neither', ax=above_below_no_neither_ax\n",
    "                )\n",
    "\n",
    "\n",
    "above_below_neither_ax = plt.subplot(2, 2, 2)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', \n",
    "                                  neither=True, relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two | Above vs. Below | With neither', ax=above_below_neither_ax\n",
    "                )\n",
    "\n",
    "\n",
    "between_no_neither_ax = plt.subplot(2, 2, 3)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', \n",
    "                                  neither=False, relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two | Between | No neither', ax=between_no_neither_ax\n",
    "                )\n",
    "\n",
    "between_neither_ax = plt.subplot(2, 2, 4)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', \n",
    "                                  neither=True, relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two | Between | With neither', ax=between_neither_ax\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 16))\n",
    "\n",
    "above_below_no_neither_ax = plt.subplot(2, 2, 1)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', \n",
    "                                  object_size=False, relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two | Above vs. Below | No object size', ax=above_below_no_neither_ax\n",
    "                )\n",
    "\n",
    "above_below_neither_ax = plt.subplot(2, 2, 2)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', \n",
    "                                  object_size=True, relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two | Above vs. Below | With object size', ax=above_below_neither_ax\n",
    "                )\n",
    "\n",
    "between_no_neither_ax = plt.subplot(2, 2, 3)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', \n",
    "                                  object_size=False, relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two | Between | No object size', ax=between_no_neither_ax\n",
    "                )\n",
    "\n",
    "between_neither_ax = plt.subplot(2, 2, 4)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='one_or_two_references', model_config='default', \n",
    "                                  object_size=True, relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='One vs. Two | Between | With object size', ax=between_neither_ax\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 6))\n",
    "\n",
    "above_below_ax = plt.subplot(1, 2, 1)\n",
    "\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias paradigm, Above vs. Below', ax=above_below_ax\n",
    "                )\n",
    "\n",
    "between_ax = plt.subplot(1, 2, 2)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias paradigm, Between', ax=between_ax\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 16))\n",
    "\n",
    "above_below_prop_ax = plt.subplot(2, 2, 1)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, plot_std=False, metric_suffix='_threshold_step', \n",
    "                 legend_loc='upper right', ylim=None, add_chance_hline=False,\n",
    "                 mean_func=lambda x: x.count() / 40, ylabel='Proportion of runs reaching 95% accuracy',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Above vs. Below | % to 95% acc ', ax=above_below_prop_ax\n",
    "                )\n",
    "\n",
    "above_below_step_ax = plt.subplot(2, 2, 2)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, plot_std=False, metric_suffix='_threshold_step', \n",
    "                 legend_loc='upper center', ylim=None, add_chance_hline=False,\n",
    "#                  mean_func=lambda x: x.count() / 40, \n",
    "                 ylabel='Mean global step to 95% accuracy',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Above vs. Below | Step to 95% acc ', ax=above_below_step_ax\n",
    "                )\n",
    "\n",
    "between_prop_ax = plt.subplot(2, 2, 3)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, plot_std=False, metric_suffix='_threshold_step', \n",
    "                 legend_loc='upper right', ylim=None, add_chance_hline=False,\n",
    "                 mean_func=lambda x: x.count() / 40, ylabel='Proportion of runs reaching 95% accuracy',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Between | % to 95% acc', ax=between_prop_ax\n",
    "                )\n",
    "\n",
    "between_step_ax = plt.subplot(2, 2, 4)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, plot_std=False, metric_suffix='_threshold_step', \n",
    "                 legend_loc='lower right', ylim=None, add_chance_hline=False,\n",
    "#                  mean_func=lambda x: x.count() / 40, \n",
    "                 ylabel='Mean global step to 95% accuracy',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Between | Step to 95% acc ', ax=between_step_ax\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 16))\n",
    "\n",
    "above_below_no_neither_ax = plt.subplot(2, 2, 1)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', \n",
    "                                  object_size=False, relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Above vs. Below | No object size', ax=above_below_no_neither_ax\n",
    "                )\n",
    "\n",
    "above_below_neither_ax = plt.subplot(2, 2, 2)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', \n",
    "                                  object_size=True, relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Above vs. Below | With object size', ax=above_below_neither_ax\n",
    "                )\n",
    "\n",
    "between_no_neither_ax = plt.subplot(2, 2, 3)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', \n",
    "                                  object_size=False, relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Between | No object size', ax=between_no_neither_ax\n",
    "                )\n",
    "\n",
    "between_neither_ax = plt.subplot(2, 2, 4)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', \n",
    "                                  object_size=True, relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Between | With object size', ax=between_neither_ax\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 16))\n",
    "\n",
    "above_below_no_neither_ax = plt.subplot(2, 2, 1)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', \n",
    "                                  neither=False, relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Above vs. Below | No neither', ax=above_below_no_neither_ax\n",
    "                )\n",
    "\n",
    "above_below_neither_ax = plt.subplot(2, 2, 2)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', \n",
    "                                  neither=True, relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Above vs. Below | With neither', ax=above_below_neither_ax\n",
    "                )\n",
    "\n",
    "between_no_neither_ax = plt.subplot(2, 2, 3)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', \n",
    "                                  neither=False, relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Between | No neither', ax=between_no_neither_ax\n",
    "                )\n",
    "\n",
    "between_neither_ax = plt.subplot(2, 2, 4)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', \n",
    "                                  neither=True, relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Between | With neither', ax=between_neither_ax\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 16))\n",
    "\n",
    "above_below_no_neither_ax = plt.subplot(2, 2, 1)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', \n",
    "                                  object_size=False, relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Above vs. Below | No object size', ax=above_below_no_neither_ax\n",
    "                )\n",
    "\n",
    "above_below_neither_ax = plt.subplot(2, 2, 2)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', \n",
    "                                  object_size=True, relation='above_below'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Above vs. Below | With object size', ax=above_below_neither_ax\n",
    "                )\n",
    "\n",
    "between_no_neither_ax = plt.subplot(2, 2, 3)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', \n",
    "                                  object_size=False, relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Between | No object size', ax=between_no_neither_ax\n",
    "                )\n",
    "\n",
    "between_neither_ax = plt.subplot(2, 2, 4)\n",
    "create_bar_chart(quinn_results_df, \n",
    "                 filter_dict=dict(paradigm='inductive_bias', model_config='default', \n",
    "                                  object_size=True, relation='between'), \n",
    "                 group_by_fields=('metric', 'model_name'),\n",
    "                 metric=None, legend_loc='lower right',\n",
    "#                  save_path='default_5_16k_group_by_relation.pdf',\n",
    "                 title='Inductive Bias | Between | With object size', ax=between_neither_ax\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open questions\n",
    "* What's the effect of adding neither\n",
    "* What's the effect of using object sizes\n",
    "* Is there a big difference in the # of epochs/steps it takes?\n",
    "\n",
    "**Make slides and send to Brenden afterwards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_gen_table = generalization_test_table(all_results_fixed_cnn_df, \n",
    "                                            filter_dict=dict(model_config='default', dataset_size=65536,\n",
    "                                                             model_name=['combined-object-mlp', 'fixed-cnn',\n",
    "                                                                         'relation-net', 'transformer']\n",
    "                                                            ), \n",
    "                                            group_by_fields=('relation', 'model_name', 'num_objects'))\n",
    "\n",
    "small_gen_table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(small_gen_table.to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_gen_table = generalization_test_table(\n",
    "    all_results_fixed_cnn_df, \n",
    "    filter_dict=dict(model_name=['combined-object-mlp', 'fixed-cnn',\n",
    "                                 'relation-net', 'transformer']\n",
    "                    ), \n",
    "    group_by_fields=('model_config', 'dataset_size', 'relation', 'model_name', 'num_objects'),\n",
    "    baseline=True)\n",
    "\n",
    "large_gen_table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_gen_table.to_latex('./figures/full_generalization_table.txt', escape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a big 'ol table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_group_by_columns = ['model_config', 'num_objects', 'dataset_size', 'relation', 'model_name']\n",
    "custom_sorted_res = all_results_df.copy()\n",
    "custom_sorted_res['model_name'] = pd.Categorical(custom_sorted_res['model_name'], \n",
    "                                                 ['combined-object-mlp', 'cnn', 'relation-net', 'transformer'])\n",
    "custom_sorted_res['relation'] = pd.Categorical(custom_sorted_res['relation'], \n",
    "                                                 ['adjacent', 'above', 'count'])\n",
    "\n",
    "custom_sorted_res = custom_sorted_res.sort_values('model_name').sort_values('relation')\n",
    "\n",
    "grouped = custom_sorted_res.groupby(table_group_by_columns, as_index=False)\n",
    "mean = grouped.mean()\n",
    "mean.to_latex('./figures/result_table.txt', \n",
    "              columns=table_group_by_columns + ['max_val_acc', 'acc_diff'],\n",
    "              float_format='%.3f', index=False, bold_rows=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df[all_results_df.model_name != 'cnn'].acc_diff.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df.acc_diff.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df[all_results_df.model_name == 'cnn'].acc_diff.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = all_results_df[all_results_df.model_config == 'larger']\n",
    "r.head()\n",
    "# r = r[r.model_name.is != 'cnn']\n",
    "# r = r[r.model_name != 'combined-object-mlp']\n",
    "# r.loc[r.max_val_acc.idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.num_objects.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.dataset_size.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.relation.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df.groupby(['model_config', 'model_name']).total_params.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_fixed_checkpoints_df[all_results_fixed_checkpoints_df.model_config != 'default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 * 2 * 3 * 4 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 * 2 * 4 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_df.groupby('model_name').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for relation in ('adjacent', 'above', 'count'):\n",
    "    d = partial_df[partial_df['relation'] == relation]\n",
    "    print(d.head())\n",
    "    print(d.groupby('model_name').max_val_acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_df.groupby(['relation', 'model_name']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'hello world'.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = all_results_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = d[d['relation'].eq('adjacent')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape, e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = e.groupby(['num_objects', 'model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(g['relation'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = g.max_val_acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.loc[(5, 'cnn')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.index.get_level_values('num_objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = r.history(samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df.val_acc.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df.iloc[3545]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df.iloc[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 => 1\n",
    "5 => 2\n",
    "8 => 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = runs[28].history()\n",
    "h2[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2.val_acc.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df.val_acc[:501].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h2.val_acc > 0.8).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2.train_acc.max() - h2.val_acc.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = api.run('simple-relational-reasoning-fixed-checkpoints/count-relation-default-models-5-objects-4096-dataset/o6p5tg8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_run(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = run.files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = list(filter(lambda f: f.name.endswith('.ckpt'), files))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.download?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = run.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'val_acc' in h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = valid_run.files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = list(files)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int([x for x in f.name.split('-') if x.startswith('epoch')][0].split('=')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax([1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
